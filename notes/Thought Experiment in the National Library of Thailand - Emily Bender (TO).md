[[index]]
- Language models like ChatGPT only produce likely sequences of words, ‘If the text makes sense it is because we, the reader, make sense of it”
- Systems cannot learn meaning but only form of language.
- The linguistic forms created by these systems don’t carry meaning except to those who know the linguistic system.
- Thought experiment: trapped in a library where everything is only in Thai and there are no images could one learn the language and understand the meaning of words? The answer is no if you do not use outside understandings like a translated book that you know in English. If you were there long enough you might be able to create a string of words a Thai person could understand but you would not actually know what it means, just like a language model does not actually know the meaning of what it produces.
- Fluency in language does not equate to intelligence.