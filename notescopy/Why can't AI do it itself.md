[[index]]
- The need for reinforcement learning human feedback
	The human cost of RLHF is clear, but what can we do about it? How can we go about this differently? There is a huge human cost for safeguards, but without the safeguards built into the models, they have the potential to spew out hateful and harmful garbage. The models need parameters, the models need guidance, and the models need human influence. The semantic touch saying this is bad, bad, terrible, bad. But if that requires human suffering, is it the right thing to do? If the fear is the technology being accessed by someone who can create/spread harmful content, is the right thing to do make guardrails or is the right thing to make it inaccessible? Where does regulation come in? Who gets to create and uphold those regulations?

- The cognitive ease paradox
	Technology emerges and is innovated from the desire to make things better for us (usually). Better (usually) means easier, and when we prioritize ease we can fall victim to many issues. My therapist told me the brain will always want to take the easier route. It's even a common application of training horses, make the right decision easy and the wrong decision hard. But when we constantly choose the easy option, we are not always opting for the right thing. The easy thing is to doomscroll on my phone, the hard thing is to read a book... is the easy thing better?
	AI has given us the ultimate ease of cognitive ability: it can simply think and assess for us. For students in a bind needing a 10,000 word essay cranked out, a prompt in ChatGPT is the easy option. For someone too busy to read through the comment section of a news post and create their own opinion, Meta's AI summarization is the easy option. We are losing critical thinking skills and the ability to outweigh the pros and cons of letting the easy option win. 

