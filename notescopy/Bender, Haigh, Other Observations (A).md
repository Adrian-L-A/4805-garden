[[index]]  

**Emily Bender, "Thought experiment in the National Library of Thailand. [link]([https://medium.com/@emilymenonbender/thought-experiment-in-the-national-library-of-thailand-f2bf761a8a83](https://medium.com/@emilymenonbender/thought-experiment-in-the-national-library-of-thailand-f2bf761a8a83))

  

Thai library experiment designed to show that ChatGPT's method of text output does not express either reasoning or meaning but are only a likely distribution.

  

*"Imagine you are in the National Library of Thailand (Thai wikipedia page). You have access to all the books in that library, except any that have illustrations or any writing not in Thai. You have unlimited time, and your physical needs are catered to, but no people to interact with. Could you learn to understand written Thai?"*

  

*"Without any way to relate the texts you are looking at to anything outside language, i.e. to hypotheses about their communicative intent, you can’t get off the ground with this task. Most of the strategies above involve pulling in additional information that would let you make those hypotheses — something beyond the strict form of the language.*

  

*You could, if you didn’t get fed up, get really good as knowing what a reasonable string of Thai “looks like”. You could maybe even write something that a Thai speaker could make sense of. But this isn’t the same thing as “knowing Thai”. If you wanted to learn from the knowledge stored in that library, you still wouldn’t have access.*

  

  

*Scale architectural model of the National Library of Thailand. Photo by Pat Roengpitya.*

*When you read the output of ChatGPT, it’s important to remember that despite its apparent fluency and despite its ability to create confident sounding strings that are on topic and seem like answers to your questions, it’s only manipulating linguistic form. It’s not understanding what you asked nor what it’s answering, let alone “reasoning” from your question + its “knowledge” to come up with the answer. The only knowledge it has is knowledge of distribution of linguistic form.*

  

*It doesn’t matter how “intelligent” it is — it can’t get to meaning if all it has access to is form. But also: it’s not “intelligent”. Our only evidence for its “intelligence” is the apparent coherence of its output. But we’re the ones doing all the meaning making there, as we make sense of it."*

  

  

  

**Thomas Haigh. “Conjoined Twins: Artificial Intelligence and the Invention of Computer Science.”[link]([https://doi.org/10.1145/3593007](https://doi.org/10.1145/3593007))

The dream of AI fundamental to the establishment of computer science; a computer that could reason better and further than a human and which might shed light on consciousness.

  

The computer was first imagined as a thinking machine, and was equated by von Neumann with an organism, which was inspired by writing comparing neurons to binary switches used in Turing machines. The essence of a brain was that it transfers information -- human brains and early computers differed in complexity but not in essence. It didn't stick, but cybernetics was born as the effort to create a mind from a machine.

  

INVENTING AI

'Artificial intelligence' first printed in 1955 by John McCarthy, who proposed to find how machines use language, abstraction, concepts, how they solve problems, and how they improve themselves. They created a program that would automate the work of logical deduction comparable to what a human mind could accomplish.

  

CREATING COMPUTER SCIENCE

AI began only as a branding approach to win funding for researchers and improve their status. It then developed under computer science labs.

  

DEMARCATING A FIELD

AI a set of techniques and approaches rather than a coherent whole.

  

  

  

**Thomas Haigh. “There Was No ‘First AI Winter.’”**

  

Conceptually, AI was about uncovering and duplicating human cognition, but practically about making computers perform tasks that humans could but machines could not. Prominent figures favoured symbolic approach where computers algorithmically manipulate symbols according to rules of logic, rather than the connectionist approach.

  

A HISTORY OF FAILED IDEAS?

AI history a cycle of hype and disappointment. A field prone to overestimation.

  

MILITARY ORIGINS OF AI

Government money needed for substantive research programs, and important milestones were reached in military programs. Money was allocated to MIT and Stanford via ARPA, which were AI centers and symbols of a military-educational complex. AI spending was small, however, but in the 60s more than the rest of the world combined. Researchers benefitted, and some believed the military program was less "intellectually corrosive" than hypothetical money from other government departments, because the military did not expend money based on deliverables. 

  

SUMMERS AND WINTERS

AI development constrained by reliance on expensive computers, the concentration of research in few labs, and money flowing from one source.

  

The failure of AI to produce technologies with clear potential. Sponsorship depended on hype for future use. Translation projects died when a government report made it seem unlikely. 

  

1980s the first winter. 1973 Lighthill Report collapsed British support, causing American funders to ask questions. James Lighthill supports industrial automation and brain function analysis, but not efforts to combine them and create machine intelligence. The US government later banned ARPA from investing in projects without specific military potential.

  

  

  

Stephen Wolfram, “What is ChatGPT Doing… And Why Does It Work?”

  

IT'S JUST ADDING ONE WORD AT A TIME

ChatGPT trying to produce a *reasonable continuation* of what text it has so far, i.e. what one would expect to someone to write after seeing what people have already written.

  

Start with "the best thing about AI is its ability to" as a phrase. Scanning billions of pages and finding all instances of this text, and using an often used next word.  When it writes an essay it is continually asking "given the text so far, what should the next word be?" 

  

Choosing the most likely word creates flat text. Choosing less likely words makes it more interesting. 

  

WHERE DO PROBABILITIES COME FROM?

First start with letter probability, it generates a string of letters. You can break that into likely chunks that imitate sentence structure, 

  

  

  

The Medium is the Message. The way we inscribe meaning, through whatever medium, opens up opportunity for _. This is amputation. 

  

What about the medium of a car? It opens up opportunities and cuts off others.

  

What is appropriate to write in a medium is an exercise in classifcation and control. Cultures that use hieroglyphs, access to that language is important. 

  

???

  

  

  

  

  

From early marks on stone to 

All have different ways to express and explore ideas. Controlling the technology of language is highly political; censorship an important part of repression, the destruction of externalized memory.

  

Writing -- the technology of remembering -- emerges all over the world early in history, and access to literacy was a coveted, jealously guarded privilege.

  

  

  

  

  

  

**SUMMARY**

-- THE MEDIUM FOR A MESSAGE is as important as the message for what is affords or permits what can be communicated but also how it extends or limits thinking about the message.

--Libraries, classification, control

--Writing and counting are intimately connected

--The infrastructure for writing has a huge impact on culture

  

  

  

The infrastructure of writing, i.e. the moveable type, first of its kind in 14th c Korea. We recall Gutenberg because there were thousands of letters in Asian forms, and Gutenberg's was therefore much more efficacious. Standardizes spelling, speeds up reading, speeds up production of information, universalizes information (by eliminating intermediaries). 

  

Reconfiguration currently of language, reality, veracity, authenticity. 

  

20th century key moment that anticipates the web and thus training data for LLMs, the invention of the memex. Vannevar Bush created analog computers by tying sequences of gears you can create a comouter to calculate tides or missile trajectories. What would it take to calculate words? Associate indexing, where any item can be caused at will to select another so numerous items have been combined into a trail. 

  

Boolian operators. Then Claude Shannon, part of a magnificent inventive stew. His MA thesis invents information theory, working out how to represent through a circuit a logical and, or, not. Boolian algebra represented via electronics. The most important MA thesis because it is the foundation of digital electronics. From the statistical property of language, t

  

entropy is a measure of what information it conveys

  

  

  

???

  

  

  

  

  

1955 -- Proposal for the Summer Research Project, budget of 150 000 USD, summoning the greatest minds to solve the big problems. The first is what can be automated, the second is what the relationship is between computers and language, the third is how neurons can be arranged to form concepts, and finally artifical life.